{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQINzPfLkaO8a/bfmqTJY6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavansai26/TRANSFORMER-NLP-MODELS/blob/main/distilbert_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DistilBERT is a smaller and faster version of the popular BERT (Bidirectional Encoder Representations from Transformers) model, developed by researchers at Hugging Face and the University of Washington.**"
      ],
      "metadata": {
        "id": "hHFKEB0FKAgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The main goal behind the creation of DistilBERT was to reduce the computational resources needed for BERT, while maintaining a similar level of performance.**"
      ],
      "metadata": {
        "id": "uq4qYLPaKJLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DistilBERT achieves this by using a technique called knowledge distillation, where a large, complex model (such as BERT) is used to train a smaller, simpler model (DistilBERT) to replicate its behavior.**"
      ],
      "metadata": {
        "id": "cloxGCvCKQqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **In this case, the BERT model is used to teach DistilBERT how to perform well on a range of natural language processing (NLP) tasks, such as text classification and question answering.**"
      ],
      "metadata": {
        "id": "3V-DH6ojKXrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **there are some key differences in the architecture of DistilBERT compared to BERT. For example, DistilBERT uses fewer layers and smaller hidden dimensions in each layer, resulting in a smaller overall model size. Additionally, it uses a technique called \"knowledge distillation\" to compress the knowledge learned by the larger BERT model into a smaller, more efficient model.**"
      ],
      "metadata": {
        "id": "YXramqnTKk0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The reduction in model size from BERT to DistilBERT is significant. BERT has 12 transformer layers and 110 million parameters, while DistilBERT has only 6 transformer layers and 66 million parameters.**"
      ],
      "metadata": {
        "id": "i2X8BcfgK4oI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Another key difference is that DistilBERT uses a \"distillation token\" to denote the start of each sentence, which helps the model learn to distinguish between different sentences within a larger block of text.**"
      ],
      "metadata": {
        "id": "vvMnFnPOKuSa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuvpnntMELBt"
      },
      "outputs": [],
      "source": []
    }
  ]
}