{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfGjBaMaWJCLE9SBblUjoY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavansai26/TRANSFORMER-NLP-MODELS/blob/main/distilbert_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DistilBERT is a smaller and faster version of the popular BERT (Bidirectional Encoder Representations from Transformers) model, developed by researchers at Hugging Face and the University of Washington.**"
      ],
      "metadata": {
        "id": "hHFKEB0FKAgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The main goal behind the creation of DistilBERT was to reduce the computational resources needed for BERT, while maintaining a similar level of performance.**"
      ],
      "metadata": {
        "id": "uq4qYLPaKJLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DistilBERT achieves this by using a technique called knowledge distillation, where a large, complex model (such as BERT) is used to train a smaller, simpler model (DistilBERT) to replicate its behavior.**"
      ],
      "metadata": {
        "id": "cloxGCvCKQqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **In this case, the BERT model is used to teach DistilBERT how to perform well on a range of natural language processing (NLP) tasks, such as text classification and question answering.**"
      ],
      "metadata": {
        "id": "3V-DH6ojKXrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **there are some key differences in the architecture of DistilBERT compared to BERT. For example, DistilBERT uses fewer layers and smaller hidden dimensions in each layer, resulting in a smaller overall model size. Additionally, it uses a technique called \"knowledge distillation\" to compress the knowledge learned by the larger BERT model into a smaller, more efficient model.**"
      ],
      "metadata": {
        "id": "YXramqnTKk0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The reduction in model size from BERT to DistilBERT is significant. BERT has 12 transformer layers and 110 million parameters, while DistilBERT has only 6 transformer layers and 66 million parameters.**"
      ],
      "metadata": {
        "id": "i2X8BcfgK4oI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Another key difference is that DistilBERT uses a \"distillation token\" to denote the start of each sentence, which helps the model learn to distinguish between different sentences within a larger block of text.**"
      ],
      "metadata": {
        "id": "vvMnFnPOKuSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **what is knowledge distillation**"
      ],
      "metadata": {
        "id": "d86Y4lYd5NCC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The basic idea behind knowledge distillation is that a large, complex model (called the \"teacher\" model) has learned a lot of information about the input data, and we want to transfer some of that knowledge to a smaller, simpler model (called the \"student\" model).**"
      ],
      "metadata": {
        "id": "VKMrnPH45hxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This is especially useful when we want to deploy the student model on a device with limited resources, such as a smartphone or a Raspberry Pi.**"
      ],
      "metadata": {
        "id": "ah0wus0d5nDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **During the knowledge distillation process, the teacher model generates \"soft targets\" in addition to the \"hard targets\" that are used in regular supervised learning. The hard targets are the true labels for each input example, while the soft targets are probabilities or logits (i.e. unnormalized log probabilities) that represent the teacher model's confidence in each class prediction.**"
      ],
      "metadata": {
        "id": "L3TdMQ6k5uAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **These soft targets are generated by passing the input examples through the teacher model, and using the output probabilities as the soft targets.**"
      ],
      "metadata": {
        "id": "iQjEhr1r5zyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The student model is then trained to match both the hard targets and the soft targets. By doing this, the student model is forced to learn not just what the correct answer is for each example, but also how to make its predictions in a way that is consistent with the teacher model's predictions.**"
      ],
      "metadata": {
        "id": "JFpmVTQq57yT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This can be especially helpful when the input data is noisy or ambiguous, since the teacher model has learned to recognize patterns and relationships that may not be immediately apparent in the raw input data.**"
      ],
      "metadata": {
        "id": "ed-Ku7aU6ArE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuvpnntMELBt"
      },
      "outputs": [],
      "source": []
    }
  ]
}