{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwdBVSZNE12KnLdQ366Tzq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavansai26/TRANSFORMER-NLP-MODELS/blob/main/ROBERTA_MODEL_INTRODUCTION_TUTORIAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1nMl6c3if-C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RoBERTa is a large-scale language model developed by Facebook AI Research (FAIR) in 2019. It is an extension of the BERT (Bidirectional Encoder Representations from Transformers) mode**"
      ],
      "metadata": {
        "id": "c85DmJE7jDJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RoBERTa stands for \"Robustly Optimized BERT approach.\"**"
      ],
      "metadata": {
        "id": "kuUzilG1jLPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The main goal of the RoBERTa model is to improve the performance of the BERT model by addressing some of its limitations. Specifically, RoBERTa is designed to address the issue of the limited amount of training data available for pre-training the BERT model.**"
      ],
      "metadata": {
        "id": "l3JkIXIlja59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RoBERTa uses an architecture similar to BERT, but it differs in some key ways. One of the main differences is the way RoBERTa is trained.**"
      ],
      "metadata": {
        "id": "edmnwwp4jh7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RoBERTa is trained using a larger corpus of text data and for a longer period of time than BERT. This allows RoBERTa to learn more about language and improve its ability to understand the nuances of language.**"
      ],
      "metadata": {
        "id": "TMpOYK9RjmYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RoBERTa also uses a modified form of the pre-training task used by BERT. In BERT, the pre-training task is a masked language modeling task, where the model is trained to predict missing words in a sentence.**"
      ],
      "metadata": {
        "id": "JyXwHKGHjtNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **In RoBERTa, the model is trained on a similar task, but the mask is randomly applied to each token in the input instead of just a fixed percentage of the tokens. This allows the model to learn more about the context in which words appear and improves its ability to understand natural language.**"
      ],
      "metadata": {
        "id": "WKspPZnvjyrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RoBERTa also uses a number of other techniques to improve its performance, such as dynamic masking, which helps to prevent overfitting, and byte-pair encoding (BPE), which is used to tokenize the input text.**"
      ],
      "metadata": {
        "id": "Y_CO3FfZkDBe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hYF3ywJPjI4x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}