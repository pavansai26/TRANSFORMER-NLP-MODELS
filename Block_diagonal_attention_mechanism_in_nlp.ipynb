{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+aPVJvOBWPUHrd6IyZYyL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavansai26/TRANSFORMER-NLP-MODELS/blob/main/Block_diagonal_attention_mechanism_in_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **what is block diagonal\" attention pattern in tranfomers**"
      ],
      "metadata": {
        "id": "W3csgtcDBDts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The Transformer network uses an attention mechanism to weigh the importance of different parts of the input sequence when processing it**"
      ],
      "metadata": {
        "id": "QLXVVZiZBQAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **In standard Transformer networks, the attention mechanism computes an attention score between each token in the input sequence and every other token.**"
      ],
      "metadata": {
        "id": "TGV8z0P0BxG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This results in a computational cost that grows quadratically with the length of the sequence. For very long sequences, this can make the model training very slow and memory-intensive.**"
      ],
      "metadata": {
        "id": "kaHDb4ZJB8Ut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **To address this issue, researchers have proposed several variants of the attention mechanism that limit the number of tokens that can attend to each other. One such variant is the \"block diagonal\" attention pattern**"
      ],
      "metadata": {
        "id": "F8JFfbK7CEU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The \"block diagonal\" attention pattern in Transformer layers refers to a specific type of attention mechanism.**"
      ],
      "metadata": {
        "id": "618Uq0MnBWit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **where the input sequence is divided into smaller blocks, and attention is only allowed within each block.**"
      ],
      "metadata": {
        "id": "D3nNewzqBd6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **In other words, attention is only allowed between tokens that are in the same block, and tokens from different blocks cannot attend to each other.**"
      ],
      "metadata": {
        "id": "a9ANL3P6Bkf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The figure below illustrates the \"block diagonal\" attention pattern for a sequence with four tokens and two blocks:**"
      ],
      "metadata": {
        "id": "_uKW4zMDCLZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input sequence: [A, B, C, D]\n",
        "Blocks: [A, B], [C, D]\n",
        "\n",
        "+---+---+     +---+---+\n",
        "| A | B | --> | A |   |\n",
        "+---+---+     +---+---+\n",
        "| C | D |     |   | D |\n",
        "+---+---+     +---+---+\n"
      ],
      "metadata": {
        "id": "LWr98xhbCWT8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7WoRLoK7805"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **In this example, the input sequence is divided into two blocks, [A, B] and [C, D].**"
      ],
      "metadata": {
        "id": "J7tNBXLZCfqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The attention mechanism only allows attention within each block, so tokens A and B can attend to each other, and tokens C and D can attend to each other. However, tokens A and C, and tokens B and D cannot attend to each other.**"
      ],
      "metadata": {
        "id": "G78VDkQ-CoOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The \"block diagonal\" attention pattern can be implemented in the Transformer network by modifying the attention mask**"
      ],
      "metadata": {
        "id": "iK1OsM0zDkV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **which is a binary matrix that indicates which tokens are allowed to attend to each other. In the standard Transformer, the attention mask is a fully-connected matrix that allows all tokens to attend to each other.**"
      ],
      "metadata": {
        "id": "LEr1uNgLEOKI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **In contrast, in the \"block diagonal\" attention pattern, the attention mask is a block diagonal matrix that only allows attention within each block.**"
      ],
      "metadata": {
        "id": "6BPdcaKEEUF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class BlockDiagonalAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, block_size):\n",
        "        super(BlockDiagonalAttention, self).__init__()\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        query, key, value = inputs\n",
        "\n",
        "        # Divide input sequence into blocks\n",
        "        batch_size, seq_length, hidden_size = query.shape.as_list()\n",
        "        num_blocks = seq_length // self.block_size\n",
        "        query = tf.reshape(query, [batch_size, num_blocks, self.block_size, hidden_size])\n",
        "        key = tf.reshape(key, [batch_size, num_blocks, self.block_size, hidden_size])\n",
        "        value = tf.reshape(value, [batch_size, num_blocks, self.block_size, hidden_size])\n",
        "\n",
        "        # Compute attention within each block\n",
        "        query = tf.transpose(query, [0, 2, 1, 3])\n",
        "        query = tf.reshape(query, [batch_size * self.block_size, num_blocks, hidden_size])\n",
        "        key = tf.transpose(key, [0, 2, 1, 3])\n",
        "        key = tf.reshape(key, [batch_size * self.block_size, num_blocks, hidden_size])\n",
        "        attention_scores = tf.matmul(query, key, transpose_b=True)\n",
        "        attention_scores = tf.reshape(attention_scores, [batch_size, self.block_size, num_blocks, self.block_size])\n",
        "        attention_scores = tf.transpose(attention_scores, [0, 2, 3, 1])\n",
        "        attention_scores = tf.reshape(attention_scores, [batch_size, seq_length, self.block_size])\n",
        "\n",
        "        # Apply attention mask\n",
        "        if mask is not None:\n",
        "            attention_scores = tf.where(mask, attention_scores, tf.constant(-1e9, dtype=attention_scores.dtype))\n",
        "\n",
        "        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n",
        "        attention_probs = tf.keras.layers.Dropout(rate=0.1)(attention_probs, training=self._in_train_mode())\n",
        "\n",
        "        # Compute weighted sum of value vectors\n",
        "        value = tf.transpose(value, [0, 2, 1, 3])\n",
        "        value = tf.reshape(value, [batch_size * self.block_size, num_blocks, hidden_size])\n",
        "        attention_probs = tf.reshape(attention_probs, [batch_size * seq_length, self.block_size, 1])\n",
        "        context_vector = tf.matmul(value, attention_probs)\n",
        "        context_vector = tf.reshape(context_vector, [batch_size, seq_length, hidden_size])\n",
        "        return context_vector\n"
      ],
      "metadata": {
        "id": "HGNbi8v9EgGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note that this implementation assumes that the query, key, and value vectors are all of the same shape. If this is not the case, you may need to adjust the reshape operations accordingly."
      ],
      "metadata": {
        "id": "x0CrIblzhW9R"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ngHDf27NhZ-t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}