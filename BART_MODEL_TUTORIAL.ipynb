{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmGR3pnbHIgKSMsH5s50aK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavansai26/TRANSFORMER-NLP-MODELS/blob/main/BART_MODEL_TUTORIAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BART (Bidirectional and Auto-Regressive Transformer) is a sequence-to-sequence (Seq2Seq) model developed by Facebook AI Research in 2019.**"
      ],
      "metadata": {
        "id": "nQEE_xV3ucAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **It is a pre-trained language model that utilizes a transformer-based architecture to generate high-quality text.**"
      ],
      "metadata": {
        "id": "4ZXpBdeOuh_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BART was designed to address some of the limitations of existing Seq2Seq models such as poor performance on long sequences, difficulty in handling multiple input formats, and lack of robustness in natural language understanding tasks.**"
      ],
      "metadata": {
        "id": "LK0_UXIauoQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The architecture of BART is similar to that of the original transformer model.**"
      ],
      "metadata": {
        "id": "iTNl9GASuujy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BART uses a bidirectional encoder, which allows it to incorporate information from both the left and right context of the input sequence. It also uses a mask-predict decoder, which enables it to generate output tokens in an auto-regressive fashion.**"
      ],
      "metadata": {
        "id": "ZR-cNYFFu6ax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BART is pre-trained using a combination of denoising autoencoding and sequence-to-sequence pre-training objectives. The denoising autoencoding task involves corrupting input sequences and training the model to reconstruct the original sequence. The sequence-to-sequence pre-training objective involves training the model to predict masked tokens in the input sequence.**"
      ],
      "metadata": {
        "id": "UxEtvIvDvHGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **what is auto regressive fashion**"
      ],
      "metadata": {
        "id": "TW8-Xx3MvV-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Auto-regressive fashion refers to the approach of generating a sequence one token at a time, where each token is conditioned on the previously generated tokens. In other words, the model generates the next token in the sequence based on the tokens it has already generated. This is in contrast to non-autoregressive models, which can generate tokens in parallel, without being conditioned on the previous tokens.**"
      ],
      "metadata": {
        "id": "W3TVCScmvbsn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Auto-regressive models are often used in language modeling and text generation tasks, where the goal is to generate a sequence of words that follows a certain pattern or structure. In these tasks, the model predicts the probability distribution over the vocabulary at each time step, and selects the most likely token based on the previously generated tokens.**"
      ],
      "metadata": {
        "id": "uHARMdjcvi__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The advantage of using an auto-regressive approach is that it allows the model to capture complex dependencies between the input and output sequence. However, it also requires the model to make assumptions about the distribution of the data, which can be a limitation in some cases. Additionally, auto-regressive models are often slower to generate output compared to non-autoregressive models, because they need to generate one token at a time.**"
      ],
      "metadata": {
        "id": "3p0TFCwIvs7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **What is bidirectional encoder**"
      ],
      "metadata": {
        "id": "e3Z_3Gj8xMke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **A bidirectional encoder is a type of neural network architecture that allows the model to incorporate information from both the left and right context of the input sequence. In a traditional uni-directional encoder, the input sequence is processed sequentially in one direction (usually left-to-right or right-to-left), and the hidden state at each time step is only influenced by the previous time steps. However, in a bidirectional encoder, the input sequence is processed in both directions simultaneously, and the hidden state at each time step is influenced by both the previous and future time steps.**"
      ],
      "metadata": {
        "id": "7Pej-m_5xavN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The bidirectional encoder works by passing the input sequence through two separate layers, one that processes the sequence from left-to-right and another that processes the sequence from right-to-left. The outputs of these two layers are then combined to form a single representation of the input sequence that incorporates information from both directions. This combined representation is then used as the input to the subsequent layers of the neural network.**"
      ],
      "metadata": {
        "id": "u0VgjbB4xjNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bidirectional encoders are commonly used in natural language processing tasks such as machine translation, text classification, and named entity recognition, where it is important to consider the context of the entire input sequence. The use of a bidirectional encoder allows the model to capture both forward and backward dependencies in the input sequence, leading to improved performance on these tasks.**"
      ],
      "metadata": {
        "id": "EwZbUWPyxrWG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AFhwNhQuXS5"
      },
      "outputs": [],
      "source": []
    }
  ]
}