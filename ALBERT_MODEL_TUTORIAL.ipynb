{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/eAWUulO/om/23DiiCUWV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavansai26/TRANSFORMER-NLP-MODELS/blob/main/ALBERT_MODEL_TUTORIAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ALBERT (A Lite BERT) is a variation of the BERT (Bidirectional Encoder Representations from Transformers) model that was introduced by Google AI Language in 2018.**"
      ],
      "metadata": {
        "id": "FbN1sXdswlz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Like BERT, ALBERT is a pre-trained transformer-based neural network architecture that achieves state-of-the-art results on a wide range of natural language processing (NLP) tasks.**"
      ],
      "metadata": {
        "id": "XZj_MVLUwo6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **WHAT IS THE MOTIVATION BEHIND ALBERT MODEL**"
      ],
      "metadata": {
        "id": "XA6spiBpw0Tn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The main motivation behind the creation of ALBERT was to reduce the number of parameters and computational complexity of the BERT model while maintaining its performance.**"
      ],
      "metadata": {
        "id": "nR5It14Lw-WW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ALBERT accomplishes this by using two main techniques:**"
      ],
      "metadata": {
        "id": "cyu9OHMHxGIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Factorized Embedding Parameterization**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2p2SHrhixNAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cross-layer Parameter Sharing**"
      ],
      "metadata": {
        "id": "YZwFREcCxSTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Factorized Embedding Parameterization**\n"
      ],
      "metadata": {
        "id": "DLJHo_kCxdTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **In BERT, the embedding layer maps each input token to a fixed-length vector representation that is then passed to the transformer layers.**"
      ],
      "metadata": {
        "id": "0s_xCCzzxnRp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The size of the embedding layer is proportional to the size of the vocabulary, which can be very large.**"
      ],
      "metadata": {
        "id": "hrnxcyb4xsy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **In ALBERT, the embedding layer is factorized into two smaller layers: one for the token embeddings and one for the positional embeddings.**"
      ],
      "metadata": {
        "id": "OORwm7lqxzWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This allows the model to use a smaller number of embedding parameters without sacrificing performance.**"
      ],
      "metadata": {
        "id": "CkwevishyAg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cross-layer Parameter Sharing:**"
      ],
      "metadata": {
        "id": "R2qvmL_7zJC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **In BERT, each transformer layer has its own set of parameters, which can make the model very large.**"
      ],
      "metadata": {
        "id": "eUjWWABGzSmM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **In ALBERT, the parameters of the transformer layers are shared across all layers. This reduces the number of parameters and allows the model to be trained more efficiently.**"
      ],
      "metadata": {
        "id": "6nXIELjJzZAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ALBERT MODEL ARCHITECTURE**"
      ],
      "metadata": {
        "id": "sD2IJj8n1IdU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The architecture of ALBERT consists of a series of transformer layers, with each layer consisting of a multi-head self-attention mechanism and a feed-forward neural network.**"
      ],
      "metadata": {
        "id": "JAfBazoT1Qkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The model uses a variant of the transformer layer known as the \"block diagonal\" attention pattern, which reduces the computational cost of the self-attention mechanism.**"
      ],
      "metadata": {
        "id": "In4poFJN1XbA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16q00UIawb0h"
      },
      "outputs": [],
      "source": []
    }
  ]
}